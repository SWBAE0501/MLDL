{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0cb8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/admin/Downloads/Deep Learning Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f0d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "네이버 경제뉴스 홈페이지 스크롤링 v4.0!!!\n",
    "저자: 육태경, 2023/04/22\n",
    "이 코드는 현제 네이버뉴스, 경제 종목포탈 홈페이지안에 존재하는 페이지링크들을 코드에 세팅된 페이지 수 많큼 'econ_homepage_links.csv'에\n",
    "같은 디렉토리안에 csv파일로 저장하는 스크롤링 프로그램입니다. *주의 이 프로그램은 굉장히 원시적인 프로그램으로, base_url의 값이 네이버 외의 포탈\n",
    "주소를 값으로 받아들이면 에러가 날 수 있습니다.\n",
    "네이버뉴스안에서만 작동하며, 광범위하게 쓰려면 다른 사이트의 url주소 특징을 파악해\n",
    "다른 알고리즘을 적용해야합니다.\n",
    "'''\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url, start_day, end_day):\n",
    "    r = session.get(url)\n",
    "    links = r.html.absolute_links\n",
    "    filtered_links = [\n",
    "        link for link in links\n",
    "        if 'https://n.news.naver.com/mnews/article' in link\n",
    "        and get_date(link) is not None\n",
    "        and start_day <= get_date(link) <= end_day\n",
    "    ]\n",
    "    return filtered_links\n",
    "\n",
    "\n",
    "def get_date(url):\n",
    "    r = session.get(url)\n",
    "    date_element = r.html.find('.t11', first=True)\n",
    "    \n",
    "    if date_element is None:\n",
    "        return None\n",
    "\n",
    "    date_str = date_element.text\n",
    "    date_obj = datetime.datetime.strptime(date_str, '%Y.%m.%d. %H:%M')\n",
    "    return date_obj.date()\n",
    "\n",
    "\n",
    "def main(start_day, end_day):\n",
    "    base_url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page='\n",
    "    num_pages = 3\n",
    "    all_links = []\n",
    "    \n",
    "    start_day = datetime.datetime.strptime(start_day, '%Y%m%d').date()\n",
    "    end_day = datetime.datetime.strptime(end_day, '%Y%m%d').date()\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url + str(page)\n",
    "        links = get_links(url, start_day, end_day)\n",
    "        all_links.extend(links)\n",
    "\n",
    "    with open('econ_hompage_links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_day = '20230420'\n",
    "    end_day = '20230422'\n",
    "    main(start_day, end_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2fd940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "네이버 경제뉴스 홈페이지 스크롤링 v3.0!!!\n",
    "저자: 육태경, 2023/04/22\n",
    "이 코드는 현제 네이버뉴스, 경제 종목포탈 홈페이지안에 존재하는 페이지링크들을 코드에 세팅된 페이지 수 많큼 'econ_homepage_links.csv'에\n",
    "같은 디렉토리안에 csv파일로 저장하는 스크롤링 프로그램입니다. *주의 이 프로그램은 굉장히 원시적인 프로그램으로, base_url의 값이 네이버 외의 포탈\n",
    "주소를 값으로 받아들이면 에러가 날 수 있습니다. 홈페이지는 현재시간으로부터 4일전까지만 뉴스내용 저장함. 5일이상 넘은것은 오류날 수 있음.\n",
    "네이버뉴스안에서만 작동하며, 광범위하게 쓰려면 다른 사이트의 url주소 특징을 파악해\n",
    "다른 알고리즘을 적용해야합니다.\n",
    "'''\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "# Create a session object for making HTTP requests\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    # Make an HTTP GET request to the specified URL\n",
    "    r = session.get(url)\n",
    "    # Extract all the links from the HTML response\n",
    "    links = r.html.absolute_links\n",
    "    # Filter the links to only include news articles\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    # Return the filtered list of links\n",
    "    return filtered_links\n",
    "\n",
    "def main():\n",
    "    # 베이스 홈페이지 주소 세팅\n",
    "    base_url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page='\n",
    "    # !!!!!!!!**중요**!!!!!!! 여기서 원하는 페이지의 양 조절 가능함.\n",
    "    num_pages = 30\n",
    "\n",
    "    # all_links라는 링크 저장소 텅빈 리스트를 만든다.\n",
    "    all_links = []\n",
    "    # 홈페이지 주소안에 있는 페이지들에 들어있는 링크들의 주소들을 all_link로 옮겨 담는다.\n",
    "    \n",
    "    #num_pages에 적혀진 수 많큼 루핑을 한다\n",
    "    for page in range(1, num_pages + 1):\n",
    "        # 현제 url주소를 구축\n",
    "        url = base_url + str(page)\n",
    "        # 현제 페이지에서 링크 주소 값 links로 옮김\n",
    "        links = get_links(url)\n",
    "        # links값을 all_links로 옮김\n",
    "        all_links.extend(links)\n",
    "\n",
    "    # 새로운 CSV 파일을 열어 링크들을 다 저장함.\n",
    "    with open('econ_hompage_links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        # 새 CSV 라이터 오브젝트를 만듬\n",
    "        writer = csv.writer(f)\n",
    "        # 헤더 열을 CSV 파일에다가 씀\n",
    "        writer.writerow(['Links'])\n",
    "        # all_link에 있는 링크들을 모두 CSV 파일에 행 데이터로 씀\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "\n",
    "# 스크립트가 성공적이었다면, main함수를 작동!\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa87405",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "네이버 경제뉴스 종목별 스크롤링 v3.0!!!\n",
    "저자: 육태경, 2023/04/24\n",
    "이 코드는 현제 네이버뉴스, 경제 종목포탈 홈페이지안에 존재하는 페이지링크들을 종목별로 코드에 세팅된 페이지 수,\n",
    "기간많큼 'econ_homepage_links.csv'에 같은 디렉토리안에 csv파일로 저장하는 스크롤링 프로그램입니다.\n",
    "*주의 이 프로그램은 굉장히 원시적인 프로그램으로, base_url의 값이 네이버 외의 포탈\n",
    "주소를 값으로 받아들이면 에러가 날 수 있습니다. 네이버뉴스안에서만 작동하며, 광범위하게 쓰려면 다른 사이트의 url주소 특징을 파악해\n",
    "다른 알고리즘을 적용해야합니다.*\n",
    "'''\n",
    "import datetime\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "# HTTP 요청을 위한 세션 오브젝트 생성\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    # 지정된 URL로 HTTP GET 요청을 보냄\n",
    "    r = session.get(url)\n",
    "    # HTML 응답에서 모든 링크를 추출\n",
    "    links = r.html.absolute_links\n",
    "    # 링크를 필터링하여 뉴스 기사만 포함\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    # 필터링된 링크 목록 반환\n",
    "    return filtered_links\n",
    "\n",
    "def main(start_day, end_day):\n",
    "    # Convert the input dates into datetime objects\n",
    "    start_day = datetime.datetime.strptime(start_day, '%Y%m%d')\n",
    "    end_day = datetime.datetime.strptime(end_day, '%Y%m%d')\n",
    "\n",
    "    # Calculate the number of days between the start and end dates\n",
    "    days_difference = (end_day - start_day).days\n",
    "\n",
    "    # 각 sid2 값에 대해 스크랩 할 페이지 수 설정\n",
    "    num_pages = 3\n",
    "\n",
    "    # 모든 링크를 저장할 빈 리스트 생성\n",
    "    all_links = []\n",
    "\n",
    "    # 각 sid2 값에 대해 반복\n",
    "    for sid2_value in [259, 258, 261, 771, 260, 262, 310, 263]:\n",
    "        # 각 sid2 값에 대한 카테고리 이름 추가\n",
    "        if sid2_value == 259:\n",
    "            all_links.append('금융')\n",
    "        elif sid2_value == 258:\n",
    "            all_links.append('증권')\n",
    "        elif sid2_value == 261:\n",
    "            all_links.append('산업/재계')\n",
    "        elif sid2_value == 771:\n",
    "            all_links.append('중기/벤처')\n",
    "        elif sid2_value == 260:\n",
    "            all_links.append('부동산')\n",
    "        elif sid2_value == 262:\n",
    "            all_links.append('글로벌 경제')\n",
    "        elif sid2_value == 310:\n",
    "            all_links.append('생활경제')\n",
    "        elif sid2_value == 263:\n",
    "            all_links.append('경제 일반')\n",
    "\n",
    "        # 현재 sid2 값에 대한 기본 URL 구성\n",
    "        base_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}'\n",
    "        # 기본 URL에서 링크 가져오기\n",
    "        links = get_links(base_url)\n",
    "        # all_links 목록에 링크 추가\n",
    "        all_links.extend(links)\n",
    "\n",
    "        # 각 페이지 번호에 대해 반복\n",
    "        for day_delta in range(days_difference + 1):\n",
    "            current_day = (start_day + datetime.timedelta(days=day_delta)).strftime('%Y%m%d')\n",
    "\n",
    "            for page in range(1, num_pages + 1):\n",
    "                # 현재 페이지 및 sid2 값에 대한 URL 구성\n",
    "                url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}&date={current_day}&page={page}'\n",
    "                # 현재 URL에서 링크 가져오기\n",
    "                links = get_links(url)\n",
    "                # all_links 목록에 링크 추가\n",
    "                all_links.extend(links)\n",
    "                \n",
    "        # 링크를 저장하기 위해 새 CSV 파일 열기\n",
    "    with open('econ_categorical_links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "        \n",
    "\n",
    "# 스크립트가 성공적이었다면, main함수를 작동!\n",
    "if __name__ == '__main__':\n",
    "    start_day = '20230420'\n",
    "    end_day = '20230422'\n",
    "    main(start_day, end_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01429337",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "네이버 경제뉴스 종목별 스크롤링 v2.0!!!\n",
    "저자: 육태경, 2023/04/22\n",
    "이 코드는 현제 네이버뉴스, 경제 종목포탈 홈페이지안에 존재하는 페이지링크들을 종목별로 코드에 세팅된 페이지 수 많큼 'econ_homepage_links.csv'에\n",
    "같은 디렉토리안에 csv파일로 저장하는 스크롤링 프로그램입니다. *주의 이 프로그램은 굉장히 원시적인 프로그램으로, base_url의 값이 네이버 외의 포탈\n",
    "주소를 값으로 받아들이면 에러가 날 수 있습니다.\n",
    "네이버뉴스안에서만 작동하며, 광범위하게 쓰려면 다른 사이트의 url주소 특징을 파악해\n",
    "다른 알고리즘을 적용해야합니다.\n",
    "'''\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "# HTTP 요청을 위한 세션 오브젝트 생성\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    # 지정된 URL로 HTTP GET 요청을 보냄\n",
    "    r = session.get(url)\n",
    "    # HTML 응답에서 모든 링크를 추출\n",
    "    links = r.html.absolute_links\n",
    "    # 링크를 필터링하여 뉴스 기사만 포함\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    # 필터링된 링크 목록 반환\n",
    "    return filtered_links\n",
    "\n",
    "# 메인 함수 정의\n",
    "def main(start_day, end_day):\n",
    "    # 각 sid2 값에 대해 스크랩 할 페이지 수 설정\n",
    "    num_pages = 3\n",
    "\n",
    "    # 모든 링크를 저장할 빈 리스트 생성\n",
    "    all_links = []\n",
    "\n",
    "    # 각 sid2 값에 대해 반복\n",
    "    for sid2_value in [259, 258, 261, 771, 260, 262, 310, 263]:\n",
    "        # 각 sid2 값에 대한 카테고리 이름 추가\n",
    "        if sid2_value == 259:\n",
    "            all_links.append('금융')\n",
    "        elif sid2_value == 258:\n",
    "            all_links.append('증권')\n",
    "        elif sid2_value == 261:\n",
    "            all_links.append('산업/재계')\n",
    "        elif sid2_value == 771:\n",
    "            all_links.append('중기/벤처')\n",
    "        elif sid2_value == 260:\n",
    "            all_links.append('부동산')\n",
    "        elif sid2_value == 262:\n",
    "            all_links.append('글로벌 경제')\n",
    "        elif sid2_value == 310:\n",
    "            all_links.append('생활경제')\n",
    "        elif sid2_value == 263:\n",
    "            all_links.append('경제 일반')\n",
    "\n",
    "            # 현재 sid2 값에 대한 기본 URL 구성\n",
    "        base_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}'\n",
    "        # 기본 URL에서 링크 가져오기\n",
    "        links = get_links(base_url)\n",
    "        # all_links 목록에 링크 추가\n",
    "        all_links.extend(links)\n",
    "\n",
    "        # 각 페이지 번호에 대해 반복\n",
    "        for page in range(2, num_pages + 1):\n",
    "            # 현재 페이지 및 sid2 값에 대한 URL 구성\n",
    "            url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}&date=20230422&page={page}'\n",
    "            # 현재 URL에서 링크 가져오기\n",
    "            links = get_links(url)\n",
    "            # all_links 목록에 링크 추가\n",
    "            all_links.extend(links)\n",
    "\n",
    "    # 링크를 저장하기 위해 새 CSV 파일 열기\n",
    "    with open('econ_categorical_links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "        \n",
    "# 스크립트가 성공적이었다면, main함수를 작동!\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ecd89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d475ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb91d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2a03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e0ff485",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    r = session.get(url)\n",
    "    links = r.html.find('.list_news a')\n",
    "    return links\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page={}'\n",
    "    num_pages = 40  # number of pages to scrape\n",
    "    links = []\n",
    "    for page in range(1, num_pages+1):\n",
    "        url = base_url + str(page)\n",
    "        links += get_links(url)\n",
    "    links = set(links)  # remove duplicates\n",
    "    with open('links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link.attrs['href']] for link in links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b96a84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#경제 홈페이지 안에 있는 종목별 스크롤링 v1.0\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url): \n",
    "    r = session.get(url)\n",
    "    links = r.html.absolute_links\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    return filtered_links\n",
    "\n",
    "def main():\n",
    "    num_pages = 3  # You can set this to the number of pages you want to scrape for each sid2 value\n",
    "    all_links = []\n",
    "\n",
    "    for sid2_value in range(259, 264):\n",
    "        base_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}'\n",
    "        links = get_links(base_url)\n",
    "        all_links.extend(links)\n",
    "\n",
    "        for page in range(2, num_pages + 1):\n",
    "            url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}&date=20230422&page={page}'\n",
    "            links = get_links(url)\n",
    "            all_links.extend(links)\n",
    "\n",
    "    with open('econ_categorical_links.csv', 'w', newline='', encoding='cp949') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb968430",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#경제 홈페이지 스크롤링 v3.0\n",
    "def main(): \n",
    "    base_url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page='\n",
    "    num_pages = 3  # You can set this to the number of pages you want to scrape\n",
    "\n",
    "    all_links = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url + str(page)\n",
    "        links = get_links(url)\n",
    "        all_links.extend(links)\n",
    "\n",
    "    with open('econ_hompage_links.csv', 'w', newline='', encoding='cp949') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6491c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    r = session.get(url)\n",
    "    links = r.html.absolute_links\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    return filtered_links\n",
    "\n",
    "def main():\n",
    "    url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page=1'\n",
    "    links = get_links(url)\n",
    "    with open('links.csv', 'w', newline='', encoding='cp949') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "891a39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    r = session.get(url)\n",
    "    links = r.html.absolute_links\n",
    "    return links\n",
    "\n",
    "def main():\n",
    "    url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101'\n",
    "    links = get_links(url)\n",
    "    with open('links.csv', 'w', newline='', encoding='cp949') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in links])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92067843",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''keywords = ['주식']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cd7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ... (Assuming you've already set up the webdriver as `driver`)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "for keyword in keywords:\n",
    "    #1. Click the search bar, clear it, and input the keyword\n",
    "    search_bar = driver.find_element(By.CSS_SELECTOR, '.search-key')\n",
    "    search_bar.click()\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    #2. Scroll and click the search result\n",
    "    while True:\n",
    "        try:\n",
    "            search_btn = driver.find_element(By.CSS_SELECTOR, '.keyword-item')\n",
    "            search_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "    \n",
    "    #3. Apply search conditions\n",
    "    while True:\n",
    "        try:\n",
    "            step3_tab = driver.find_element(By.CSS_SELECTOR, '.btn.btn-search.news-search-btn.news-report-search-btn')\n",
    "            step3_tab.click()\n",
    "            break\n",
    "        except:\n",
    "            driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "    \n",
    "    #4. Scroll and click Step 3: Analyze results and visualization\n",
    "    while True:\n",
    "        try:\n",
    "            excel_btn = driver.find_element(By.CSS_SELECTOR, '.btn-collap-group.btn-collap')\n",
    "            excel_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            driver.execute_script(\"window.scrollBy(0, -300)\")\n",
    "    \n",
    "    #5. Click the download button\n",
    "    while True:\n",
    "        try:\n",
    "            close_btn = driver.find_element(By.CSS_SELECTOR, '.btn.btn-wh.ml-auto.news-download-btn.mobile-excel-download')\n",
    "            close_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''time.sleep(1)\n",
    "\n",
    "for keyword in keywords:\n",
    "    #1. 검색어 창을 클릭해서 검색어를 입력\n",
    "    search_bar = driver.find_element_by_class_name('search-key')\n",
    "    search_bar.click()\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    #2. 스크롤을 내려서 검색 결과 클릭\n",
    "    while True:\n",
    "        try:\n",
    "            search_btn = driver.find_element_by_class_name('keyword-item')\n",
    "            search_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            scroll(-300)\n",
    "    #3. 검색 조건 적용\n",
    "    while True:\n",
    "        try:\n",
    "            step3_tab = driver.find_element_by_class_name('btn btn-search news-search-btn news-report-search-btn')\n",
    "            step3_tab.click()\n",
    "            break\n",
    "        except:\n",
    "            scroll(-300)\n",
    "    #4. 스크롤해서 스텝 3:분석 결과 및 시각화 클릭\n",
    "    while True:\n",
    "        try:\n",
    "            excel_btn = driver.find_element_by_class_name('btn-collap-group btn-collap')\n",
    "            excel_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            scroll(-300)\n",
    "    #5. 다운로드 버튼 클릭\n",
    "    while True:\n",
    "        try:\n",
    "            close_btn = driver.find_element_by_class_name('btn btn-wh ml-auto news-download-btn mobile-excel-download')\n",
    "            close_btn.click()\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "    time.sleep(1)\n",
    "    step1_tab = driver.find_element_by_class_name('tap_step1')\n",
    "    step1_tab.click()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc53091",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def scroll(line):\n",
    "    '''\n",
    "    '''아래로 스크롤 : -\n",
    "    위로 스크롤 : +'''\n",
    "    '''\n",
    "    time.sleep(0.5)\n",
    "    pyautogui.scroll(line)\n",
    "    time.sleep(0.5)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348fc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import csv\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Replace the example URL with the specific website you want to scrape\n",
    "url = \"https://www.bigkinds.or.kr/v2/news/search.do\"\n",
    "\n",
    "session = HTMLSession()\n",
    "response = session.get(url)\n",
    "\n",
    "# Replace '.news-link' with the appropriate CSS selector for the links you want to extract\n",
    "news_links = response.html.find('.news-detail')\n",
    "\n",
    "# Extract the URLs and store them in a list\n",
    "urls = [link.attrs['href'] for link in news_links]\n",
    "\n",
    "# Save the URLs to a CSV file\n",
    "with open('news_links.csv', 'w', newline='', encoding='cp949') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for url in urls:\n",
    "        writer.writerow([url])'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44422063",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "네이버 경제뉴스 종목별 스크롤링 v2.0!!!\n",
    "저자: 육태경, 2023/04/22\n",
    "이 코드는 현제 네이버뉴스, 경제 종목포탈 홈페이지안에 존재하는 페이지링크들을 종목별로 코드에 세팅된 페이지 수 많큼 'econ_homepage_links.csv'에\n",
    "같은 디렉토리안에 csv파일로 저장하는 스크롤링 프로그램입니다. *주의 이 프로그램은 굉장히 원시적인 프로그램으로, base_url의 값이 네이버 외의 포탈\n",
    "주소를 값으로 받아들이면 에러가 날 수 있습니다.\n",
    "네이버뉴스안에서만 작동하며, 광범위하게 쓰려면 다른 사이트의 url주소 특징을 파악해\n",
    "다른 알고리즘을 적용해야합니다.\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import csv\n",
    "\n",
    "# Create a session object for making HTTP requests\n",
    "session = HTMLSession()\n",
    "\n",
    "def get_links(url):\n",
    "    # Make an HTTP GET request to the specified URL\n",
    "    r = session.get(url)\n",
    "    # Extract all the links from the HTML response\n",
    "    links = r.html.absolute_links\n",
    "    # Filter the links to only include news articles\n",
    "    filtered_links = [link for link in links if 'https://n.news.naver.com/mnews/article' in link]\n",
    "    # Return the filtered list of links\n",
    "    return filtered_links\n",
    "\n",
    "# Define the main function\n",
    "def main():\n",
    "    # Set the number of pages to scrape for each sid2 value\n",
    "    num_pages = 3\n",
    "\n",
    "    # Create an empty list to store all the links\n",
    "    all_links = []\n",
    " \n",
    "    # Loop through each sid2 value\n",
    "    for sid2_value in [259, 258, 261, 771, 260, 262, 310, 263]:\n",
    "        # Add category names for each sid2 value\n",
    "        if sid2_value == 259:\n",
    "            all_links.append('금융')\n",
    "        elif sid2_value == 258:\n",
    "            all_links.append('증권')\n",
    "        elif sid2_value == 261:\n",
    "            all_links.append('산업/재계')\n",
    "        elif sid2_value == 771:\n",
    "            all_links.append('중기/벤처')\n",
    "        elif sid2_value == 260:\n",
    "            all_links.append('부동산')\n",
    "        elif sid2_value == 262:\n",
    "            all_links.append('글로벌 경제')\n",
    "        elif sid2_value == 310:\n",
    "            all_links.append('생활경제')\n",
    "        elif sid2_value == 263:\n",
    "            all_links.append('경제 일반')\n",
    "\n",
    "\n",
    "\n",
    "        # Construct the base URL for the current sid2 value\n",
    "        base_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}'\n",
    "        # Get the links from the base URL\n",
    "        links = get_links(base_url)\n",
    "        # Add the links to the all_links list\n",
    "        all_links.extend(links)\n",
    "\n",
    "        # Loop through each page number\n",
    "        for page in range(2, num_pages + 1):\n",
    "            # Construct the URL for the current page and sid2 value\n",
    "            url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101&sid2={sid2_value}&date=20230422&page={page}'\n",
    "            # Get the links from the current URL\n",
    "            links = get_links(url)\n",
    "            # Add the links to the all_links list\n",
    "            all_links.extend(links)\n",
    "\n",
    "    # Open a new CSV file to save the links\n",
    "    with open('econ_categorical_links.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Links'])\n",
    "        writer.writerows([[link] for link in all_links])\n",
    "        \n",
    "# 스크립트가 성공적이었다면, main함수를 작동!\n",
    "if __name__ == '__main__':\n",
    "    main()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
